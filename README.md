# 数据处理demo

## 1. 概述

本项目为一个综合性的数据处理流程提供了一个分步演示，旨在指导初学者处理用于深度学习的大规模科学数据集。工作流从原始的 NetCDF (`.nc`) 文件开始（这是气候学和海洋学中常见的格式），并将其转换为适用于训练 PyTorch 模型的高效格式。

整个过程分为两个主要阶段，每个阶段都在其自己的 Jupyter Notebook 中有详细说明：

1.  **阶段一：原始数据预处理 (`dataProcess.ipynb`)**
    *   **目标**: 检查、分析并将庞大的原始 NetCDF 文件转换为结构更清晰、更易于访问的 HDF5 (`.h5`) 格式。
    *   **流程**: 这个 Notebook 涵盖了读取 `.nc` 文件、计算统计数据、可视化数据分布，以及使用滑动窗口方法创建更小、更易于管理的数据切片。

2.  **阶段二：面向任务的准备与模型训练 (`taskOriented_dataProcess.ipynb`)**
    *   **目标**: 为特定的深度学习任务（例如，超分辨率）准备 HDF5 数据，并演示一个完整的训练和评估周期。
    *   **流程**: 这个 Notebook 读取 `.h5` 文件，生成低质量样本，使用 `webdataset` 将数据打包成 `.tar` 分片以实现高效加载，构建 PyTorch `DataLoaders`，并训练一个简单的 CNN 模型。

## 2. 项目结构

```
dataProcess_demo/
├── README.md                       # 本文档
├── dataProcess_README.md           # 阶段1示例README
├── taskOriented_dataProcess.md     # 阶段2示例README
├── dataProcess.ipynb               # Notebook 1: 从原始 .nc 到处理后的 .h5
├── taskOriented_dataProcess.ipynb  # Notebook 2: 从 .h5 到 .tar 分片和模型训练
├── demo.nc                         # 示例原始 NetCDF 数据文件
└── ... (其他生成的文件，如图片、stats.json)
```

## 3. 工作流程与使用方法

请按顺序执行以下两个阶段以复现完整的流程。

### 第 1 部分：原始数据处理 (`dataProcess.ipynb`)

该 Notebook 指导您完成将原始数据转换为干净、已切片的 HDF5 文件的初始步骤。

**关键步骤:**

1.  **设置**: 将您的原始数据文件（例如 `demo.nc`）放入 `demo` 目录中。
2.  **数据检查**: Notebook 会打开 `.nc` 文件并检查其变量、维度和属性。
3.  **统计分析**: 计算并可视化关键统计数据（均值、标准差、分位数）和数据分布，以帮助您理解数据集。
4.  **切片 (Patching)**: 使用滑动窗口算法从大的全局数据阵列中提取较小的切片（例如 `448x448`）。这是为模型创建样本的关键步骤。
    *   **循环填充**: 处理全球经度数据的日界线回绕问题。
    *   **数据过滤**: 根据可配置的阈值，丢弃大部分为陆地的切片。
5.  **保存为 HDF5**: 所有有效的切片及其对应的纬度、经度和掩码数据都被保存到一个单一的、压缩的 HDF5 文件 (`demo.h5`) 中。这种格式支持高效的部分 I/O，对于大型数据集至关重要。

**要亲自运行，请打开 `dataProcess.ipynb` 并按顺序执行所有单元格。请确保更新文件路径以指向您自己的数据。**

### 第 2 部分：任务准备与模型训练 (`taskOriented_dataProcess.ipynb`)

该 Notebook 读取处理好的 `demo.h5` 文件，为超分辨率任务做准备，然后训练一个模型。

**关键步骤:**

1.  **数据加载**: 从 `demo.h5` 文件中读取切片数据。
2.  **生成低质量 (LQ) 数据**: 为了超分辨率任务，通过对高分辨率的真实数据 (GT) 进行降采样和上采样来创建低质量输入。这模拟了模型的低分辨率输入。
3.  **使用 WebDataset 打包**: 将数据（GT、LQ、经度、纬度、掩码、日期）打包成 `.tar` 文件（分片）。`webdataset` 是一个高性能的 I/O 库，非常适合从磁盘流式传输大型数据集，避免将所有内容加载到内存中。
4.  **训练/验证/测试集划分**: `.tar` 分片会根据基于时间的比例自动划分为 `train`、`val` 和 `test` 子目录。
5.  **创建 DataLoader**: 在 `webdataset` 分片之上构建 PyTorch `DataLoader`。这包括：
    *   **归一化**: 使用从训练集中计算出的统计数据进行 Z-score 归一化。
    *   **批处理 (Batching)**: 将样本分组为批次以供模型使用。
6.  **模型训练与评估**:
    *   定义了一个简单的卷积神经网络 (CNN)。
    *   演示了一个完整的训练循环，包括前向传播、损失计算（忽略陆地上的 `NaN` 值）、反向传播和优化。
    *   展示了验证和测试循环以评估模型性能。

**要亲自运行，请首先确保 `demo.h5` 已由第 1 部分生成。然后，打开 `taskOriented_dataProcess.ipynb` 并执行所有单元格。请更新任何必要的目录路径。**

## 4. 主要依赖库

要运行这些 Notebook，您将需要以下主要库。您可以通过 `pip` 或 `conda` 安装它们。

-   `numpy`
-   `h5py`
-   `netCDF4`
-   `matplotlib`
-   `tqdm`
-   `torch`
-   `webdataset`
-   `opencv-python` (用于 `cv2`)

## 5. 自定义注意事项

在将此流程应用于您自己的项目时，请特别注意以下几点：

*   **文件路径**: 这是最常见的错误来源。请务必仔细检查输入数据和输出目录的路径是否正确。

*   **处理参数**: 在 `dataProcess.ipynb` 中调整 `PATCH_SIZE`、`STRIDE` 和 `LAND_THRESHOLD` 等参数以满足您的需求。
*   **任务特定逻辑**: `taskOriented_dataProcess.ipynb` 中的 LQ 数据生成是为超分辨率任务设计的。如果您的任务不同（例如，分割、预测），您需要相应地修改此逻辑。

*   **归一化**: 务必**仅从您的训练集**计算归一化统计数据（均值/标准差），并将其应用于所有数据集（训练、验证、测试），以防止数据泄露。